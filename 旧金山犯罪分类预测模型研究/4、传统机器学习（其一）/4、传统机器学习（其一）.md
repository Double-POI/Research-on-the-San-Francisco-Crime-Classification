# [旧金山犯罪分类预测模型研究]4、传统机器学习（其一）

这是一篇专门用来灌水的部分，在这一篇里面，我们将使用传统机器学习的方法建立sf-crime的模型

**灌水部分**，不调参，只灌水

导入数据

```python
import numpy as np
from sklearn.metrics import log_loss

train_features = np.load('./data/train_features.npy')
train_labels = np.load('./data/train_labels.npy')
test_features = np.load('./data/test_features.npy')
```

## 朴素贝叶斯分类

```python
from sklearn.naive_bayes import BernoulliNB
model = BernoulliNB()
model.fit(train_features, train_labels)
predicted = np.array(model.predict_proba(train_features))
print ("朴素贝叶斯的log损失为 %f" % (log_loss(train_labels, predicted)))
```

得到输出：

朴素贝叶斯的log损失为 2.582578

## 逻辑回归

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(C=0.1)
model.fit(train_features, train_labels)
predicted = np.array(model.predict_proba(train_features))
print ("逻辑回归的log损失为 %f" % (log_loss(train_labels, predicted)))
```

得到输出：

逻辑回归的log损失为 2.540033

在运行过程中，遇到了“STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.”错误，逻辑回归未能在合理的迭代次数内收敛，由于后面将使用神经网络进行分析，神经网络能更好的完成模型的训练，不予逻辑回归进一步调参。


## 决策树

```python
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth = 5)
model.fit(train_features, train_labels)
predicted = np.array(model.predict_proba(train_features))
print ("决策树的log损失为 %f" % (log_loss(train_labels, predicted)))
```

得到输出：

决策树的log损失为 2.543998

在调试过程中发现决策树极易产生过拟合现象，由于后面将使用梯度提升树（Gradient Boosting Decision Tree,GBDT）进行分析，梯度提升树能更好的完成模型的训练，不予决策树进一步调参。

## 随机森林

```python
from sklearn.ensemble import RandomForestClassifier  
model = RandomForestClassifier(max_leaf_nodes = 5,n_estimators = 10)
#随手写的参数
model.fit(train_features, train_labels)
predicted = np.array(model.predict_proba(train_features))
print ("随机森林的log损失为 %f" % (log_loss(train_labels, predicted)))
```

得到输出：

随机森林的log损失为 2.597370

在调试过程中发现随机森林易产生过拟合现象且内存耗费大，由于后面将使用梯度提升树（Gradient Boosting Decision Tree,GBDT）进行分析，梯度提升树能更好的完成模型的训练，不予随机森林进一步调参。


## 梯度提升决策树--以lightGBM为例

```python
import lightgbm as lgb
model = lgb.LGBMClassifier(num_leaves=35,learning_rate=0.05,n_estimators=20)
#随手写的参数
model.fit(train_features, train_labels)
predicted = np.array(model.predict_proba(train_features))
print ("lightGBM的log损失为 %f" % (log_loss(train_labels, predicted)))
```

得到输出：

lightGBM的log损失为 2.509984

进行提交：

```python
import pandas as pd
testResult = np.array(model.predict_proba(test_features))
sampleSubmission = pd.read_csv('../input/sf-crime/sampleSubmission.csv.zip')
Result_pd = pd.DataFrame(testResult,
                         index=sampleSubmission.index,
                         columns=sampleSubmission.columns[1:])
Result_pd.to_csv('../working/sampleSubmission(test).csv', index_label='Id')
```

测试集误差：

![](1.png)

梯度提升树将在后文中进一步调优。

此外，还尝试了SVM算法，发现由于数据集的问题，很难找到最优超平面，遂不再展示。
