# [旧金山犯罪分类预测模型研究]8、多层神经网络（其二）

接上上一篇的结尾，导入数据、定义sf_crime类：


```python
import pandas as pd
import numpy as np
import torch
from torch import nn
from sklearn.model_selection import KFold

train_features = np.load('./data/train_features.npy')
train_labels = np.load('./data/train_labels_onehot.npy')
test_features = np.load('./data/test_features.npy')

num_inputs = 21
num_outputs = 39

class sf_crime():
    def __init__(self, num_epochs, k_fold_num, batch_size, k_fold):
        self.num_epochs = num_epochs
        self.k_fold_num = k_fold_num
        self.batch_size = batch_size
        self.k_fold = k_fold
        self.run()

    def make_iter(self, train_features, train_labels):
        train_features = torch.tensor(train_features, dtype=torch.float).to(device)
        train_labels = torch.tensor(train_labels).to(device)
        dataset = torch.utils.data.TensorDataset(train_features, train_labels)
        return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=True)

    def show_loss(self, features, labels, team):
        net.eval()
        batch = self.make_iter(features, labels)
        loss_num = 0
        n = 0
        for x, y in batch:
            loss_num += loss(net(x), y).sum().item()
            n += 1
        print(team, end=' ')
        print('loss:', loss_num / n)

    def train(self, features, labels):
        net.train()
        train_iter = self.make_iter(features, labels)
        for X, y in train_iter:
            y_hat = net(X)
            l = loss(y_hat, y).sum()
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
        self.show_loss(features, labels, '训练集')

    def run(self):
        if self.k_fold:
            kf = KFold(n_splits=self.k_fold_num, shuffle=True)
            for epoch in range(self.num_epochs):
                fold_num = 0
                for train_index, test_index in kf.split(train_features):
                    X_train, X_test = train_features[train_index], train_features[
                        test_index]
                    y_train, y_test = train_labels[train_index], train_labels[
                        test_index]
                    print('第%d轮的第%d折：' % (epoch + 1, fold_num + 1))
                    fold_num += 1
                    self.train(X_train, y_train)
                    self.show_loss(X_test, y_test, '测试集')
        else:
            for epoch in range(self.num_epochs):
                print('第%d轮：' % (epoch + 1))
                self.train(train_features, train_labels)

    def write(self, version):
        net.eval()
        test_iter = torch.utils.data.DataLoader(torch.tensor(test_features,
                                                             dtype=torch.float).to(device),
                                                1024,
                                                shuffle=False)
        testResult = [line for x in test_iter for line in net(x).cpu().detach().numpy()]
        sampleSubmission = pd.read_csv('../input/sf-crime/sampleSubmission.csv.zip')
        Result_pd = pd.DataFrame(testResult,
                                 index=sampleSubmission.index,
                                 columns=sampleSubmission.columns[1:])
        Result_pd.to_csv('../working/sampleSubmission('+str(version)+').csv', index_label='Id')
        torch.save(net, '../working/net('+str(version)+').pkl')
        print('Finish!')
        
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class MultiClassLogLoss(torch.nn.Module):
    def __init__(self):
        super(MultiClassLogLoss, self).__init__()

    def forward(self, y_pred, y_true):
        return -(y_true *
                 torch.log(y_pred.float() + 1.00000000e-15)) / y_true.shape[0]

loss = MultiClassLogLoss().to(device)
```

## 引入ADAM算法

Adam 是一种可以替代传统随机梯度下降（SGD）过程的一阶优化算法，它能基于训练数据迭代地更新神经网络权重。具有如下优势：

1.直截了当地实现

2.高效的计算

3.所需内存少

4.梯度对角缩放的不变性、

5.适合解决含大规模数据和参数的优化问题

6.适用于非稳态（non-stationary）目标

7.适用于解决包含很高噪声或稀疏梯度的问题

8.超参数可以很直观地解释，并且基本上只需极少量的调参

适度的调大batch_size，加快训练：

```python
class build_model(torch.nn.Module):
    def __init__(self, num_inputs, num_outputs):
        super(build_model, self).__init__()
        self.net = torch.nn.Sequential()
        self.net.add_module('Linear', nn.Linear(num_inputs, num_outputs))
        self.net.add_module('Softmax', nn.Softmax(dim=-1))

    def forward(self, x):
        return self.net(x)

net = build_model(num_inputs, num_outputs).to(device)

optimizer = torch.optim.Adam(net.parameters(), lr = 0.1)

sf_crime_1 = sf_crime(num_epochs = 3, k_fold_num = 6, batch_size = 1024, k_fold = True)
```

&nbsp; | 训练集 loss | 测试集 loss
:-: | :-: | :-:
第1轮的第1折 | 2.5714463850834988 | 2.5684672702442515
第1轮的第2折 | 2.576505300215074 | 2.579311692631328
第1轮的第3折 | 2.56927140642713 | 2.5692834620709184
第1轮的第4折 | 2.5726130095395177 | 2.5766285732909515
第1轮的第5折 | 2.5695439518748464 | 2.5632351228407213
第1轮的第6折 | 2.5716270606834573 | 2.5772705244851277
第2轮的第1折 | 2.5717541221138482 | 2.5788582571736582
第2轮的第2折 | 2.5699191717001106 | 2.5719893612228075
第2轮的第3折 | 2.5735102210011513 | 2.5745553886973775
第2轮的第4折 | 2.5786073544642307 | 2.5801492787741283
第2轮的第5折 | 2.57283088677413 | 2.567786668564056
第2轮的第6折 | 2.5682252960605223 | 2.5647884648996633
第3轮的第1折 | 2.5689177659841684 | 2.5679092207155025
第3轮的第2折 | 2.567440762553182 | 2.569945113642232
第3轮的第3折 | 2.572759691318432 | 2.5698793734703864
第3轮的第4折 | 2.577294387350549 | 2.578237691959301
第3轮的第5折 | 2.570175269266942 | 2.5723287125567458
第3轮的第6折 | 2.5706060166125533 | 2.5746588857023864

## 增加隐藏层并添加激活函数PReLU

层数的增加能有效的提升神经网络的拟合能力，但是多层Dense层以全连接的方式相连接，仍然等同于一层，需要引入激活函数。

参数化修正线性单元（Parameteric Rectified Linear Unit，PReLU）属于 ReLU 修正类激活函数的一员。它和 RReLU 以及 Leaky ReLU 有一些共同点，即为负值输入添加了一个线性项。而最关键的区别是，这个线性项的斜率实际上是在模型训练中学习到的。

降低学习率lr：

```python
class build_model(torch.nn.Module):
    def __init__(self, num_inputs, num_outputs):
        super(build_model, self).__init__()
        self.net = torch.nn.Sequential()
        self.net.add_module('Linear1', nn.Linear(num_inputs, 256))
        self.net.add_module('PReLU1', nn.PReLU(256))
        self.net.add_module('Linear-out', nn.Linear(256, num_outputs))
        self.net.add_module('Softmax', nn.Softmax(dim=-1))

    def forward(self, x):
        return self.net(x)

net = build_model(num_inputs, num_outputs).to(device)

optimizer = torch.optim.Adam(net.parameters(), lr = 0.05)

sf_crime_2 = sf_crime(num_epochs = 3, k_fold_num = 6, batch_size = 1024, k_fold = True)
```

&nbsp; | 训练集 loss | 测试集 loss
:-: | :-: | :-:
第1轮的第1折 | 2.560995357686823 | 2.5636703234452467
第1轮的第2折 | 2.560400128864742 | 2.566296585789927
第1轮的第3折 | 2.5435580500355965 | 2.5449448682211497
第1轮的第4折 | 2.536481381796457 | 2.52774959177404
第1轮的第5折 | 2.5378770478121884 | 2.541401341244891
第1轮的第6折 | 2.5385033090631444 | 2.5405327306760777
第2轮的第1折 | 2.5233555086842783 | 2.521572156385942
第2轮的第2折 | 2.5283460170239 | 2.5291790261968865
第2轮的第3折 | 2.529217211516587 | 2.531825642485719
第2轮的第4折 | 2.5321651495420014 | 2.53450096903981
第2轮的第5折 | 2.5248642624674975 | 2.5277930823239414
第2轮的第6折 | 2.525798779934436 | 2.5294350827490533
第3轮的第1折 | 2.5223946211221335 | 2.5233165034047373
第3轮的第2折 | 2.526981333752612 | 2.52607898945575
第3轮的第3折 | 2.526863006111625 | 2.533160741512592
第3轮的第4折 | 2.5271550465297032 | 2.532358574700522
第3轮的第5折 | 2.5443091789325636 | 2.538275351891151
第3轮的第6折 | 2.539229459695883 | 2.540893953163307

## Dropout层及BN算法

若单纯增加层数不加以限制，会导致过拟合现象的发生。

dropout可以让模型训练时，随机让网络的某些节点输出置零，也不更新权重，其他过程不变。从而减少过拟合的现象。

BN层是对于每个神经元做归一化处理，可以起到如下作用：

1、改善流经网络的梯度；

2、允许更大的学习率，提高训练速度；

3、减少对初始化的强烈依赖；

4、改善正则化策略，轻微减少了对dropout的需求；

6、提高网络泛化能力；

7、不需要使用使用局部响应归一化层了；

8、可以把训练数据彻底打乱。

```python
class build_model(torch.nn.Module):
    def __init__(self, num_inputs, num_outputs, hn=64, dp=0.5, layers=2):
        super(build_model, self).__init__()
        self.net = torch.nn.Sequential()
        self.net.add_module('Linear1', nn.Linear(num_inputs, hn))
        self.net.add_module('PReLU1', nn.PReLU(hn))
        self.net.add_module('Dropout1', nn.Dropout(p=dp))
        for i in range(layers):
            self.net.add_module('Linear' + str(i + 2), nn.Linear(hn, hn))
            self.net.add_module('PReLU' + str(i + 2), nn.PReLU(hn))
            self.net.add_module('BatchNorm' + str(i + 1), nn.BatchNorm1d(hn))
            self.net.add_module('Dropout' + str(i + 2), nn.Dropout(p=dp))
        self.net.add_module('Linear-out', nn.Linear(hn, num_outputs))
        self.net.add_module('Softmax', nn.Softmax(dim=-1))

    def forward(self, x):
        return self.net(x)

net = build_model(num_inputs, num_outputs).to(device)

optimizer = torch.optim.Adam(net.parameters(), lr = 0.05)

sf_crime_3 = sf_crime(num_epochs = 3, k_fold_num = 6, batch_size = 1024, k_fold = True)
```

&nbsp; | 训练集 loss | 测试集 loss
:-: | :-: | :-:
第1轮的第1折 | 2.56638293999892 | 2.562827185317353
第1轮的第2折 | 2.558674452521584 | 2.562333337076894
第1轮的第3折 | 2.5553139136387752 | 2.5554991201920942
第1轮的第4折 | 2.5519849093643936 | 2.553815353166807
第1轮的第5折 | 2.559399174643563 | 2.564164853596187
第1轮的第6折 | 2.55796268236387 | 2.555022451427433
第2轮的第1折 | 2.555549265454699 | 2.555476402069305
第2轮的第2折 | 2.555449908930105 | 2.5593174537578665
第2轮的第3折 | 2.5539399793931654 | 2.551660310971987
第2轮的第4折 | 2.552227055609643 | 2.5512160838067115
第2轮的第5折 | 2.566546737230741 | 2.5664986213604055
第2轮的第6折 | 2.5491769980717374 | 2.5525687074327803
第3轮的第1折 | 2.5510908320233536 | 2.5498190142891626
第3轮的第2折 | 2.552627453437218 | 2.5575146274966793
第3轮的第3折 | 2.549942904585725 | 2.548543086418739
第3轮的第4折 | 2.5563663349284993 | 2.5596473083629476
第3轮的第5折 | 2.55066615851609 | 2.5511944394011596
第3轮的第6折 | 2.548675792867487 | 2.545277478811624



## 残差层

随着网络层数的增加，训练难度上升，部分层出现梯度消失，浅层的参数无法更新。

可以定义残差层来抑制网络退化：

```python
class Residual(nn.Module):
    def __init__(self, num_inputs, num_outputs):
        super(Residual, self).__init__()
        self.middle_L = nn.Linear(num_inputs, num_outputs)
        self.middle_R = nn.ReLU(num_outputs)
        if num_inputs != num_outputs:
            self.right = nn.Linear(num_inputs, num_outputs)
        else:
            self.right = None
        self.middle_B = nn.BatchNorm1d(num_outputs)
    def forward(self, X):
        Y = self.middle_B(self.middle_R(self.middle_L(X)))
        if self.right:
            X = self.right(X)
        return Y + X
    
```

降低学习率lr：

```python
class build_model(nn.Module):
    def __init__(self, num_inputs, num_outputs, dp=0.5):
        super(build_model, self).__init__()
        self.net = nn.Sequential()
        self.net.add_module('Residual1', Residual(num_inputs, 1024))
        self.net.add_module('Residual2', Residual(1024, 512))
        self.net.add_module('Residual3', Residual(512, 512))
        self.net.add_module('Residual4', Residual(512, 256))
        self.net.add_module('Residual5', Residual(256, 256))
        self.net.add_module('Residual6', Residual(256, 128))
        self.net.add_module('Residual7', Residual(128, 128))
        self.net.add_module('Residual8', Residual(128, 64))
        self.net.add_module('Residual9', Residual(64, 64))
        self.net.add_module('Linear-out', nn.Linear(64, num_outputs))
        self.net.add_module('Softmax', nn.Softmax(dim=-1))
    def forward(self, x):
        return self.net(x)
    
net = build_model(num_inputs, num_outputs).to(device)

optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)

sf_crime_4 = sf_crime(num_epochs = 3, k_fold_num = 6, batch_size = 1024, k_fold = True)
```

&nbsp; | 训练集 loss | 测试集 loss
:-: | :-: | :-:
第1轮的第1折 | 2.5189488190871017 | 2.5260313390851854
第1轮的第2折 | 2.513177054078429 | 2.518192304597868
第1轮的第3折 | 2.5007555911591 | 2.5121760801835493
第1轮的第4折 | 2.4861165626899346 | 2.491209493650423
第1轮的第5折 | 2.4690725643318014 | 2.4683254148576643
第1轮的第6折 | 2.4476417331428793 | 2.4567126744276995
第2轮的第1折 | 2.4210602110082453 | 2.4312199912704786
第2轮的第2折 | 2.4077808860298635 | 2.41906263944986
第2轮的第3折 | 2.398388769243147 | 2.41259841485457
第2轮的第4折 | 2.397047152552571 | 2.4044536777309604
第2轮的第5折 | 2.389288325743242 | 2.3966387618671763
第2轮的第6折 | 2.372035514057933 | 2.3938787950502407
第3轮的第1折 | 2.3670885936363595 | 2.3810817961926225
第3轮的第2折 | 2.358098377214445 | 2.3811613179586986
第3轮的第3折 | 2.3503227817428693 | 2.3700029499880917
第3轮的第4折 | 2.3469226140242356 | 2.374108921397816
第3轮的第5折 | 2.332383739364731 | 2.3483906792594005
第3轮的第6折 | 2.3233934685900497 | 2.3481683847787496

误差降得快，训练效果好，进行输出:

```python
net = build_model(num_inputs, num_outputs).to(device)
sf_crime_5 = sf_crime(num_epochs = 75, k_fold_num = 5, batch_size = 128, k_fold = False)
sf_crime_5.write('v2')
# 此部分数据是上交到kaggle后由GPU跑出，参数未变，但由于缺少计算资源，在CPU上运行时间太长，加之代码、硬件变化，不予复现
```

&nbsp; | 训练集 loss | &nbsp; | 训练集 loss 
:-: | :-: | :-: | :-:
第1轮 | 2.532128783372732 | 第2轮 | 2.5117432226389993
第3轮 | 2.4987093857674054 | 第4轮 | 2.4650355627486755
第5轮 | 2.44225353989012 | 第6轮 | 2.432890730860072
第7轮 | 2.4221955523902166 | 第8轮 | 2.4226611791512904
第9轮 | 2.4129180616432135 | 第10轮 | 2.41405027535134
第11轮 | 2.4008438345435614 | 第12轮 | 2.40186852369553
第13轮 | 2.396401159691088 | 第14轮 | 2.392729426875259
第15轮 | 2.3865575167960498 | 第16轮 | 2.3903897981265763
第17轮 | 2.391178473448142 | 第18轮 | 2.390286771011797
第19轮 | 2.38301262099704 | 第20轮 | 2.3805301756291954
第21轮 | 2.3761558679989725 | 第22轮 | 2.3768800711020446
第23轮 | 2.359234025428345 | 第24轮 | 2.3556665871248934
第25轮 | 2.3613409704261725 | 第26轮 | 2.3609874215159383
第27轮 | 2.341428651676311 | 第28轮 | 2.342289823474306
第29轮 | 2.3466516399716997 | 第30轮 | 2.3290268788526665
第31轮 | 2.344144325156312 | 第32轮 | 2.3238662164528052
第33轮 | 2.3338206492381772 | 第34轮 | 2.316863007478781
第35轮 | 2.3163462466015403 | 第36轮 | 2.304803479801525
第37轮 | 2.3276602871768124 | 第38轮 | 2.3012462073550637
第39轮 | 2.296868058629247 | 第40轮 | 2.292901530132427
第41轮 | 2.2828861566690297 | 第42轮 | 2.299591307039861
第43轮 | 2.2943672689246686 | 第44轮 | 2.2703809335237337
第45轮 | 2.288071677401349 | 第46轮 | 2.261207561670761
第47轮 | 2.2592816775217477 | 第48轮 | 2.245214062415081
第49轮 | 2.270078106360002 | 第50轮 | 2.2344410322207113
第51轮 | 2.2556811551669815 | 第52轮 | 2.262334980608978
第53轮 | 2.256335735876799 | 第54轮 | 2.236338401452089
第55轮 | 2.2431761026382446 | 第56轮 | 2.2201628512713736
第57轮 | 2.199509350292055 | 第58轮 | 2.1961013437151076
第59轮 | 2.1945995867669166 | 第60轮 | 2.214539958880498
第61轮 | 2.191579738141218 | 第62轮 | 2.1802288388316726
第63轮 | 2.2214432590769166 | 第64轮 | 2.2117651773221567
第65轮 | 2.164032549569101 | 第66轮 | 2.1991856762301394
第67轮 | 2.1562348823725204 | 第68轮 | 2.18965525727172
第69轮 | 2.1484812295242346 | 第70轮 | 2.145705941951636
第71轮 | 2.1530181847530088 | 第72轮 | 2.154164569916981
第73轮 | 2.159680441542939 | 第74轮 | 2.1619253325295613
第75轮 | 2.161952587552282 | 


提交，发现过拟合：

![](1.png)

## 降低过拟合

增加两个Dropout层来降低过拟合：

```python
class build_model(nn.Module):
    def __init__(self, num_inputs, num_outputs, dp=0.5):
        super(build_model, self).__init__()
        self.net = nn.Sequential()
        self.net.add_module('Residual1', Residual(num_inputs, 1024))
        self.net.add_module('Residual2', Residual(1024, 512))
        self.net.add_module('Residual3', Residual(512, 512))
        self.net.add_module('Residual4', Residual(512, 256))
        self.net.add_module('Dropout1', nn.Dropout(dp))
        self.net.add_module('Residual5', Residual(256, 256))
        self.net.add_module('Residual6', Residual(256, 128))
        self.net.add_module('Residual7', Residual(128, 128))
        self.net.add_module('Residual8', Residual(128, 64))
        self.net.add_module('Dropout2', nn.Dropout(dp))
        self.net.add_module('Residual9', Residual(64, 64))
        self.net.add_module('Linear-out', nn.Linear(64, num_outputs))
        self.net.add_module('Softmax', nn.Softmax(dim=-1))
    def forward(self, x):
        return self.net(x)
    
net = build_model(num_inputs, num_outputs).to(device)

optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)

sf_crime_6 = sf_crime(num_epochs = 3, k_fold_num = 6, batch_size = 1024, k_fold = True)
```

&nbsp; | 训练集 loss | 测试集 loss
:-: | :-: | :-:
第1轮的第1折 | 2.531314628441017 | 2.537703082278058
第1轮的第2折 | 2.522075258935248 | 2.5217196908030477
第1轮的第3折 | 2.51781182089052 | 2.5205326630518985
第1轮的第4折 | 2.5116197255941537 | 2.5127988778627834
第1轮的第5折 | 2.499079208440714 | 2.499604763684573
第1轮的第6折 | 2.4888608285597154 | 2.496323132014775
第2轮的第1折 | 2.4723022034118225 | 2.4704653833295915
第2轮的第2折 | 2.4573255052099694 | 2.4650287711536967
第2轮的第3折 | 2.450022388338209 | 2.4555471510320275
第2轮的第4折 | 2.44492984051471 | 2.4528587331305016
第2轮的第5折 | 2.4407840758770494 | 2.4463681807884803
第2轮的第6折 | 2.4352504493473295 | 2.4402960563873077
第3轮的第1折 | 2.429513962952407 | 2.4368271877715637
第3轮的第2折 | 2.422563133373127 | 2.4347961399105045
第3轮的第3折 | 2.4207374719473034 | 2.425267246219662
第3轮的第4折 | 2.416952591342526 | 2.4215744291985786
第3轮的第5折 | 2.406511242072899 | 2.4143979249300656
第3轮的第6折 | 2.4068318183605486 | 2.4127483217866272


训练效果好，进行输出：

```python
net = build_model(num_inputs, num_outputs).to(device)
sf_crime_7 = sf_crime(num_epochs = 100, k_fold_num = 5, batch_size = 128, k_fold = False)
sf_crime_7.write('v4')
# 此部分数据是上交到kaggle后由GPU跑出，参数未变，但由于缺少计算资源，在CPU上运行时间太长，加之代码、硬件变化，不予复现
# notebook:https://www.kaggle.com/doublepoi/a-nn-with-residual-v4
# 但已保留训练好的net一个，位于/working目录下
```

&nbsp; | 训练集 loss | &nbsp; | 训练集 loss 
:-: | :-: | :-: | :-:
第1轮 | 2.5501436031225957 | 第2轮 | 2.5336308323697887
第3轮 | 2.5169244718440487 | 第4轮 | 2.4984171940730167
第5轮 | 2.4717182229448866 | 第6轮 | 2.465790965062477
第7轮 | 2.4520922863122188 | 第8轮 | 2.4594474944757136
第9轮 | 2.448417986745323 | 第10轮 | 2.452351965270676
第11轮 | 2.4401685263449218 | 第12轮 | 2.4455038250743093
第13轮 | 2.440071613360674 | 第14轮 | 2.4367397567886853
第15轮 | 2.438828974614888 | 第16轮 | 2.4294834039705893
第17轮 | 2.437890118096536 | 第18轮 | 2.4299771646797517
第19轮 | 2.426350261503722 | 第20轮 | 2.4352396501527798
第21轮 | 2.4266955160594486 | 第22轮 | 2.433283886153659
第23轮 | 2.4258396931183643 | 第24轮 | 2.425638135218676
第25轮 | 2.431690213563559 | 第26轮 | 2.4265123366753816
第27轮 | 2.4237702723705405 | 第28轮 | 2.430981558241766
第29轮 | 2.416090435637183 | 第30轮 | 2.4186721792865744
第31轮 | 2.414435958528852 | 第32轮 | 2.4289439046855295
第33轮 | 2.418597133565338 | 第34轮 | 2.4151861734323568
第35轮 | 2.4096448196397793 | 第36轮 | 2.4229075853085464
第37轮 | 2.417416958820014 | 第38轮 | 2.4095014339282517
第39轮 | 2.416745287277204 | 第40轮 | 2.423104208666128
第41轮 | 2.416455941322522 | 第42轮 | 2.4155340000188157
第43轮 | 2.4129015618548806 | 第44轮 | 2.4141191577577925
第45轮 | 2.418196903519975 | 第46轮 | 2.4060689788876157
第47轮 | 2.4013931267745012 | 第48轮 | 2.411806288576904
第49轮 | 2.4095945180435003 | 第50轮 | 2.399239064374448
第51轮 | 2.409775673926293 | 第52轮 | 2.4015270896724887
第53轮 | 2.412829833708721 | 第54轮 | 2.4032281095331367
第55轮 | 2.412689936188829 | 第56轮 | 2.4004261665966684
第57轮 | 2.404295102699653 | 第58轮 | 2.4092094159070707
第59轮 | 2.399979714469198 | 第60轮 | 2.4046222744010266
第61轮 | 2.3959598499578196 | 第62轮 | 2.3921851947868897
第63轮 | 2.389886306993889 | 第64轮 | 2.4009636760035873
第65轮 | 2.3931640897875344 | 第66轮 | 2.3873726040611176
第67轮 | 2.382343913291718 | 第68轮 | 2.4012956741528635
第69轮 | 2.388719952745593 | 第70轮 | 2.386917024781376
第71轮 | 2.3855074366211615 | 第72轮 | 2.3884354337945686
第73轮 | 2.382631479721247 | 第74轮 | 2.3797537846998735
第75轮 | 2.369646894070374 | 第76轮 | 2.3796484612084767
第77轮 | 2.3817930546673862 | 第78轮 | 2.364137144355507
第79轮 | 2.371351284858508 | 第80轮 | 2.3756885414634827
第81轮 | 2.3689271750705783 | 第82轮 | 2.3680319274777855
第83轮 | 2.3811245945505886 | 第84轮 | 2.3698978023929196
第85轮 | 2.3771264139033144 | 第86轮 | 2.367808127736712
第87轮 | 2.3636441328030924 | 第88轮 | 2.3725308199306747
第89轮 | 2.3520693201007266 | 第90轮 | 2.3584287980775454
第91轮 | 2.35885754442993 | 第92轮 | 2.3588727815445765
第93轮 | 2.3647255825273916 | 第94轮 | 2.3540118221358544
第95轮 | 2.3447853235098033 | 第96轮 | 2.344675707094597
第97轮 | 2.361060996433516 | 第98轮 | 2.3487158052293293
第99轮 | 2.3404077983402707 | 第100轮 | 2.361993959733656



提交得到测试集误差：

![](2.png)
